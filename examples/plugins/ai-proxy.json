{
  "entity_checks": [
    {
      "conditional": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "bedrock",
            "gemini"
          ]
        },
        "then_err": "bedrock and gemini only support auth.allow_override = false",
        "then_field": "auth.allow_override",
        "then_match": {
          "eq": false
        }
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "llama2"
          ]
        },
        "then_at_least_one_of": [
          "model.options.llama2_format"
        ],
        "then_err": "must set %s for llama2 provider"
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "mistral"
          ]
        },
        "then_at_least_one_of": [
          "model.options.mistral_format"
        ],
        "then_err": "must set %s for mistral provider"
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "anthropic"
          ]
        },
        "then_at_least_one_of": [
          "model.options.anthropic_version"
        ],
        "then_err": "must set %s for anthropic provider"
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "azure"
          ]
        },
        "then_at_least_one_of": [
          "model.options.azure_instance"
        ],
        "then_err": "must set %s for azure provider"
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "azure"
          ]
        },
        "then_at_least_one_of": [
          "model.options.azure_api_version"
        ],
        "then_err": "must set %s for azure provider"
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "azure"
          ]
        },
        "then_at_least_one_of": [
          "model.options.azure_deployment_id"
        ],
        "then_err": "must set %s for azure provider"
      }
    },
    {
      "conditional_at_least_one_of": {
        "if_field": "model.provider",
        "if_match": {
          "one_of": [
            "llama2"
          ]
        },
        "then_at_least_one_of": [
          "model.options.upstream_url"
        ],
        "then_err": "must set %s for self-hosted providers/models"
      }
    },
    {
      "custom_entity_check": {
        "field_sources": [
          "logging",
          "model",
          "route_type"
        ]
      }
    },
    {
      "custom_entity_check": {
        "field_sources": [
          "route_type"
        ]
      }
    },
    {
      "custom_entity_check": {
        "field_sources": [
          "model",
          "route_type"
        ]
      }
    },
    {
      "custom_entity_check": {
        "field_sources": [
          "model",
          "route_type"
        ]
      }
    },
    {
      "mutually_required": [
        "auth.header_name",
        "auth.header_value"
      ]
    },
    {
      "mutually_required": [
        "auth.param_location",
        "auth.param_name",
        "auth.param_value"
      ]
    }
  ],
  "fields": [
    {
      "auth": {
        "fields": [
          {
            "allow_override": {
              "default": false,
              "description": "If enabled, the authorization header or parameter can be overridden in the request by the value configured in the plugin.",
              "required": false,
              "type": "boolean"
            }
          },
          {
            "aws_access_key_id": {
              "description": "Set this if you are using an AWS provider (Bedrock) and you are authenticating using static IAM User credentials. Setting this will override the AWS_ACCESS_KEY_ID environment variable for this plugin instance.",
              "encrypted": true,
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "aws_secret_access_key": {
              "description": "Set this if you are using an AWS provider (Bedrock) and you are authenticating using static IAM User credentials. Setting this will override the AWS_SECRET_ACCESS_KEY environment variable for this plugin instance.",
              "encrypted": true,
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "azure_client_id": {
              "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client ID.",
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "azure_client_secret": {
              "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client secret.",
              "encrypted": true,
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "azure_tenant_id": {
              "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the tenant ID.",
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "azure_use_managed_identity": {
              "default": false,
              "description": "Set true to use the Azure Cloud Managed Identity (or user-assigned identity) to authenticate with Azure-provider models.",
              "required": false,
              "type": "boolean"
            }
          },
          {
            "gcp_service_account_json": {
              "description": "Set this field to the full JSON of the GCP service account to authenticate, if required. If null (and gcp_use_service_account is true), Kong will attempt to read from environment variable `GCP_SERVICE_ACCOUNT`.",
              "encrypted": true,
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "gcp_use_service_account": {
              "default": false,
              "description": "Use service account auth for GCP-based providers and models.",
              "required": false,
              "type": "boolean"
            }
          },
          {
            "header_name": {
              "description": "If AI model requires authentication via Authorization or API key header, specify its name here.",
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "header_value": {
              "description": "Specify the full auth header value for 'header_name', for example 'Bearer key' or just 'key'.",
              "encrypted": true,
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "param_location": {
              "description": "Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.",
              "one_of": [
                "body",
                "query"
              ],
              "required": false,
              "type": "string"
            }
          },
          {
            "param_name": {
              "description": "If AI model requires authentication via query parameter, specify its name here.",
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          },
          {
            "param_value": {
              "description": "Specify the full parameter value for 'param_name'.",
              "encrypted": true,
              "referenceable": true,
              "required": false,
              "type": "string"
            }
          }
        ],
        "required": false,
        "type": "record"
      }
    },
    {
      "genai_category": {
        "default": "text/generation",
        "description": "Generative AI category of the request",
        "one_of": [
          "audio/speech",
          "audio/transcription",
          "image/generation",
          "text/embeddings",
          "text/generation",
          "video/generation"
        ],
        "required": false,
        "type": "string"
      }
    },
    {
      "llm_format": {
        "default": "openai",
        "description": "LLM input and output format and schema to use",
        "one_of": [
          "bedrock",
          "cohere",
          "gemini",
          "huggingface",
          "openai"
        ],
        "required": false,
        "type": "string"
      }
    },
    {
      "logging": {
        "fields": [
          {
            "log_payloads": {
              "default": false,
              "description": "If enabled, will log the request and response body into the Kong log plugin(s) output.",
              "required": true,
              "type": "boolean"
            }
          },
          {
            "log_statistics": {
              "default": false,
              "description": "If enabled and supported by the driver, will add model usage and token metrics into the Kong log plugin(s) output.",
              "required": true,
              "type": "boolean"
            }
          }
        ],
        "required": true,
        "type": "record"
      }
    },
    {
      "max_request_body_size": {
        "default": 8192,
        "description": "max allowed body size allowed to be introspected. 0 means unlimited, but the size of this body will still be limited by Nginx's client_max_body_size.",
        "gt": 0,
        "type": "integer"
      }
    },
    {
      "model": {
        "fields": [
          {
            "name": {
              "description": "Model name to execute.",
              "required": false,
              "type": "string"
            }
          },
          {
            "options": {
              "description": "Key/value settings for the model",
              "fields": [
                {
                  "anthropic_version": {
                    "description": "Defines the schema/API version, if using Anthropic provider.",
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "azure_api_version": {
                    "default": "2023-05-15",
                    "description": "'api-version' for Azure OpenAI instances.",
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "azure_deployment_id": {
                    "description": "Deployment ID for Azure OpenAI instances.",
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "azure_instance": {
                    "description": "Instance name for Azure OpenAI hosted models.",
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "bedrock": {
                    "entity_checks": [
                      {
                        "mutually_required": [
                          "aws_assume_role_arn",
                          "aws_role_session_name"
                        ]
                      }
                    ],
                    "fields": [
                      {
                        "aws_assume_role_arn": {
                          "description": "If using AWS providers (Bedrock) you can assume a different role after authentication with the current IAM context is successful.",
                          "required": false,
                          "type": "string"
                        }
                      },
                      {
                        "aws_region": {
                          "description": "If using AWS providers (Bedrock) you can override the `AWS_REGION` environment variable by setting this option.",
                          "required": false,
                          "type": "string"
                        }
                      },
                      {
                        "aws_role_session_name": {
                          "description": "If using AWS providers (Bedrock), set the identifier of the assumed role session.",
                          "type": "string"
                        }
                      },
                      {
                        "aws_sts_endpoint_url": {
                          "description": "If using AWS providers (Bedrock), override the STS endpoint URL when assuming a different role.",
                          "type": "string"
                        }
                      },
                      {
                        "embeddings_normalize": {
                          "default": false,
                          "description": "If using AWS providers (Bedrock), set to true to normalize the embeddings.",
                          "type": "boolean"
                        }
                      },
                      {
                        "performance_config_latency": {
                          "description": "Force the client's performance configuration 'latency' for all requests. Leave empty to let the consumer select the performance configuration.",
                          "required": false,
                          "type": "string"
                        }
                      }
                    ],
                    "required": false,
                    "type": "record"
                  }
                },
                {
                  "cohere": {
                    "fields": [
                      {
                        "embedding_input_type": {
                          "default": "classification",
                          "description": "The purpose of the input text to calculate embedding vectors.",
                          "one_of": [
                            "classification",
                            "clustering",
                            "image",
                            "search_document",
                            "search_query"
                          ],
                          "required": false,
                          "type": "string"
                        }
                      },
                      {
                        "wait_for_model": {
                          "description": "Wait for the model if it is not ready",
                          "required": false,
                          "type": "boolean"
                        }
                      }
                    ],
                    "required": false,
                    "type": "record"
                  }
                },
                {
                  "embeddings_dimensions": {
                    "description": "If using embeddings models, set the number of dimensions to generate.",
                    "gt": 0,
                    "required": false,
                    "type": "integer"
                  }
                },
                {
                  "gemini": {
                    "entity_checks": [
                      {
                        "mutually_required": [
                          "api_endpoint",
                          "location_id",
                          "project_id"
                        ]
                      }
                    ],
                    "fields": [
                      {
                        "api_endpoint": {
                          "description": "If running Gemini on Vertex, specify the regional API endpoint (hostname only).",
                          "required": false,
                          "type": "string"
                        }
                      },
                      {
                        "location_id": {
                          "description": "If running Gemini on Vertex, specify the location ID.",
                          "required": false,
                          "type": "string"
                        }
                      },
                      {
                        "project_id": {
                          "description": "If running Gemini on Vertex, specify the project ID.",
                          "required": false,
                          "type": "string"
                        }
                      }
                    ],
                    "required": false,
                    "type": "record"
                  }
                },
                {
                  "huggingface": {
                    "fields": [
                      {
                        "use_cache": {
                          "description": "Use the cache layer on the inference API",
                          "required": false,
                          "type": "boolean"
                        }
                      },
                      {
                        "wait_for_model": {
                          "description": "Wait for the model if it is not ready",
                          "required": false,
                          "type": "boolean"
                        }
                      }
                    ],
                    "required": false,
                    "type": "record"
                  }
                },
                {
                  "input_cost": {
                    "description": "Defines the cost per 1M tokens in your prompt.",
                    "gt": 0,
                    "required": false,
                    "type": "number"
                  }
                },
                {
                  "llama2_format": {
                    "description": "If using llama2 provider, select the upstream message format.",
                    "one_of": [
                      "ollama",
                      "openai",
                      "raw"
                    ],
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "max_tokens": {
                    "description": "Defines the max_tokens, if using chat or completion models.",
                    "required": false,
                    "type": "integer"
                  }
                },
                {
                  "mistral_format": {
                    "description": "If using mistral provider, select the upstream message format.",
                    "one_of": [
                      "ollama",
                      "openai"
                    ],
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "output_cost": {
                    "description": "Defines the cost per 1M tokens in the output of the AI.",
                    "gt": 0,
                    "required": false,
                    "type": "number"
                  }
                },
                {
                  "temperature": {
                    "between": [
                      0,
                      5
                    ],
                    "description": "Defines the matching temperature, if using chat or completion models.",
                    "required": false,
                    "type": "number"
                  }
                },
                {
                  "top_k": {
                    "between": [
                      0,
                      500
                    ],
                    "description": "Defines the top-k most likely tokens, if supported.",
                    "required": false,
                    "type": "integer"
                  }
                },
                {
                  "top_p": {
                    "between": [
                      0,
                      1
                    ],
                    "description": "Defines the top-p probability mass, if supported.",
                    "required": false,
                    "type": "number"
                  }
                },
                {
                  "upstream_path": {
                    "deprecation": {
                      "message": "llm: config.model.options.upstream_path is deprecated, please use config.model.options.upstream_url instead",
                      "removal_in_version": "4.0"
                    },
                    "description": "Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.",
                    "required": false,
                    "type": "string"
                  }
                },
                {
                  "upstream_url": {
                    "description": "Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.",
                    "required": false,
                    "type": "string"
                  }
                }
              ],
              "required": false,
              "type": "record"
            }
          },
          {
            "provider": {
              "description": "AI provider request format - Kong translates requests to and from the specified backend compatible formats.",
              "one_of": [
                "anthropic",
                "azure",
                "bedrock",
                "cohere",
                "gemini",
                "huggingface",
                "llama2",
                "mistral",
                "openai"
              ],
              "required": true,
              "type": "string"
            }
          }
        ],
        "required": true,
        "type": "record"
      }
    },
    {
      "model_name_header": {
        "default": true,
        "description": "Display the model name selected in the X-Kong-LLM-Model response header",
        "type": "boolean"
      }
    },
    {
      "response_streaming": {
        "default": "allow",
        "description": "Whether to 'optionally allow', 'deny', or 'always' (force) the streaming of answers via server sent events.",
        "one_of": [
          "allow",
          "always",
          "deny"
        ],
        "required": false,
        "type": "string"
      }
    },
    {
      "route_type": {
        "description": "The model's operation implementation, for this provider. ",
        "one_of": [
          "audio/v1/audio/speech",
          "audio/v1/audio/transcriptions",
          "audio/v1/audio/translations",
          "image/v1/images/edits",
          "image/v1/images/generations",
          "llm/v1/assistants",
          "llm/v1/batches",
          "llm/v1/chat",
          "llm/v1/completions",
          "llm/v1/embeddings",
          "llm/v1/files",
          "llm/v1/responses",
          "preserve",
          "realtime/v1/realtime"
        ],
        "required": true,
        "type": "string"
      }
    }
  ],
  "required": true,
  "type": "record"
}